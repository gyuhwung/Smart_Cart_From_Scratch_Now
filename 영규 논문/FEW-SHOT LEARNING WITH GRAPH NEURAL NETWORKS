https://underdogy.tistory.com/2

1. 논문 요약

 

해당 논문은 ICLR 2018에서 발표된 퓨샷러닝(few shot learning, few shot classification) 분야에서 그래프 뉴럴 네트워크를 활용하여 타 모델에 비해 좋은 성능을 냈었다는 내용이다. 해당 논문은 GNN을 포함한 기존 모델에 대한 소개와 few shot learning, semi supervised learning, active learning을 모두 적용해 보았다. 

 

2. 배경지식

 

2.1 GNN (Graph Neural Network), GCN(Graph Convolution Network)

GNN이 일반 Neural Network와 다른 점은 input data이다. 일반 뉴럴넷은 input data만 필요하다면, GNN에서는 convolution 연산을 위해 input data(X 라 하자)뿐만 아니라, 데이터 어떻게 연결되어있는지에 대한 정보(adjacent data : A라 하자)가 필요하다.

2개의 데이터셋과 가중치(W) 매트릭스를 연산하여 output이 나오는 형태이다.


3. 논문 살펴보기

3.1 모델 구조

매번 convolution 레이어를 거칠때마다 adjacency matrix는 재계산되며 마지막으로 나온 그래프 값을 통해서 이미지를 분류하는 것으로써 아래와 같이 softmax function을 통해 데이터를 예측한다. 목적함수를 최적화하는 과정에서 layer마다 존재하는 A,X,W가 연쇄적으로 최적화되는 구조이다.

 



목적함수
논문에서 활용한 GNN 구조는 아래와 같다.

1X{3X3-conv. layer (64 filters), batch normalization, max pool(2,2), leaky relu},
1X{3X3-conv. layer (96 filters), batch normalization, max pool(2,2), leaky relu},
1X{3X3-conv. layer (128 filters), batch normalization, max pool(2,2), leaky relu, dropout(0:5)},
1X{3X3-conv. layer (256 filters), batch normalization, max pool(2,2), leaky relu, dropout(0:5)},
1X{fc-layer (128 filters), batch normalization}

 

4개의 3x3 콘볼루션 레이어(각 레이어가 64, 96, 128, 256개의 필터로 이루어져 있음)와 이후 배치 노멀라이제이션, 풀링, 리키렐루를 활용했고, 3,4번 레이어의 경우 마지막에 드롭아웃을 활용하여 일반화의 성능을 높였다. 마지막 레이어는 풀리커넥티드 레이어(128개 필터)와 배치 노멀라이제이션을 통해 값을 예측하였다.

 

3.2 모델 성능


omniglot 데이터셋에서의 성능 결과 비교

mini imagenet에서의 성능 결과 비교
논문에서는 2가지의 데이터셋(omniglot, mini-imagenet)에서 기존의 모델들과 성능을 비교했다. 5Way 1shot과 5shot에서 그리고 20Way 1shot과 5shot에서 모두 높은 결과를 보여주었다. 기존에 높은 성능을 보였주었던 siamese net, Matching net, meta net보다 퓨샷러닝 테스크에서 높은 성능을 보여주었다는 것은 활용가치가 있어보이며, GNN이라는 input데이터 형태가 어떤 노드간의 연결 데이터를 포함했다는 점이 다른 모델에 비해 의미가 컸다고 생각한다.
